{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MLP:\n",
    "    '''Multilayer perceptron class.\n",
    "    \n",
    "    Parameters:\n",
    "    layers -- Sequence of ints giving the number of neurons in each layer.\n",
    "    \n",
    "    weights -- Sequence of (len(layer) - 1) numpy matrices W. W[l][j][i] gives the weight\n",
    "        of the synaptic connection from neuron i of layer l to neuron j of layer (l+1).\n",
    "    \n",
    "    transfer_functions -- Sequence of (len(layer) - 1) numpy vectorized function objects\n",
    "        specifying the transfer function used by the neurons of each non-input layer.\n",
    "        Defaults to the hyperbolic tangent if left unspecified.\n",
    "    \n",
    "    transfer_derivatives -- Sequence of (len(layer) - 1) numpy vectorized function objects\n",
    "        giving the transfer function derivatives of the neurons of each non-input layer.\n",
    "        Defaults to the hyperbolic tangent derivative if left unspecified.\n",
    "    \n",
    "    learning_rate -- Learning rate for backpropagation. Defaults to 0.1.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, layers, weights=None, bias=None, transfer_functions=None, transfer_derivatives=None, learning_rate=0.1):\n",
    "        self.layers = layers\n",
    "        self.input_dim = layers[0]\n",
    "        self.output_dim = layers[-1]\n",
    "        self.num_layers = len(layers)\n",
    "        \n",
    "        # A level is the part of the network made of two adjacent layers together with the synaptic connections between them.\n",
    "        self.levels = [(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.num_levels = self.num_layers - 1\n",
    "        \n",
    "        self.weights = weights if weights else [np.zeros((level[1], level[0])) for level in self.levels]\n",
    "        self.bias = bias if bias else [np.zeros(level[1]) for level in self.levels]\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if transfer_functions is not None:\n",
    "            self.transfer_functions = transfer_functions\n",
    "        else:\n",
    "            self.transfer_functions = [np.tanh] * self.num_levels\n",
    "            \n",
    "        if transfer_derivatives is not None:\n",
    "            self.transfer_derivatives = transfer_derivatives\n",
    "        else:\n",
    "            self.transfer_derivatives = [lambda x: 1.0 - np.tanh(x)**2] * self.num_levels\n",
    "        \n",
    "        \n",
    "    def validate(self):\n",
    "        '''Check consistency of network parameters.'''\n",
    "        \n",
    "        \n",
    "    def seed_weights(self, lower=-1.0, upper=1.0, seed=None):\n",
    "        '''Initialize weights to random uniformly distributed reals in the half-open interval [lower, upper).\n",
    "        Optionally seed the random number generator.\n",
    "        '''\n",
    "        \n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        self.weights = [(np.random.rand(level[1], level[0]) - 0.5) * (upper - lower) for level in self.levels]\n",
    "        \n",
    "        \n",
    "    def seed_bias(self, lower=-1.0, upper=1.0, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "        self.bias = [(np.random.rand(level[1]) - 0.5) * (upper - lower) for level in self.levels]\n",
    "    \n",
    "    \n",
    "    def feed(self, input_vector):\n",
    "        output = input_vector\n",
    "        for i, W in enumerate(self.weights):\n",
    "            output = self.transfer_functions[i](np.dot(W, output) + self.bias[i])\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def backpropagate(self, training_input, training_output):\n",
    "        '''Adjust weights using backpropagation on one training vector.'''\n",
    "        \n",
    "        # Feed training input through the network and get the resulting output.\n",
    "        net_values = [None] * self.num_levels  # Used to store the net values of the output neurons of each level.\n",
    "        output_values = [None] * self.num_levels  # Store the actual output values (i.e. the net value passed through the transfer function) of the output neurons of each level.\n",
    "        \n",
    "        output = training_input\n",
    "        for i, W in enumerate(self.weights):\n",
    "            net = np.dot(W, output) + self.bias[i]  # Vector of net values (i.e. weighted sum of synaptic inputs + bias) of the output neurons.\n",
    "            output = self.transfer_functions[i](net)  # Output vector of this level.\n",
    "            # Store net and output for backprop later:\n",
    "            net_values[i] = net\n",
    "            output_values[i] = output\n",
    "            \n",
    "        # Backpropagate to get the deltas, weight and bias changes.\n",
    "        deltas = [None] * self.num_levels\n",
    "        weight_changes = [None] * self.num_levels\n",
    "        bias_changes = [None] * self.num_levels\n",
    "        # Output layer:\n",
    "        deltas[-1] = (training_output - output) * self.transfer_derivatives[-1](net_values[-1])\n",
    "        weight_changes[-1] = self.learning_rate * np.outer(deltas[-1], output_values[-2])\n",
    "        bias_changes[-1] = self.learning_rate * deltas[-1]\n",
    "        # Hidden layers:\n",
    "        for j in range(self.num_levels-2, 0, -1):\n",
    "            deltas[j] = np.dot(deltas[j+1], self.weights[j+1]) * self.transfer_derivatives[j](net_values[j])\n",
    "            weight_changes[j] = self.learning_rate * np.outer(deltas[j], output_values[j-1])\n",
    "            bias_changes[j] = self.learning_rate * deltas[j]\n",
    "        # First level:\n",
    "        deltas[0] = np.dot(deltas[1], self.weights[1]) * self.transfer_derivatives[0](net_values[0])\n",
    "        weight_changes[0] = self.learning_rate * np.outer(deltas[0], training_input)\n",
    "        bias_changes[0] = self.learning_rate * deltas[0]\n",
    "        \n",
    "        # Finally adjust bias and weights.\n",
    "        self.weights = [self.weights[i] + weight_changes[i] for i in range(self.num_levels)]\n",
    "        self.bias = [self.bias[i] + bias_changes[i] for i in range(self.num_levels)]\n",
    "        \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"MLP({0}, {1})\".format(self.layers, self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd0c45d55f8>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XOR problem\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mlp = MLP([2,2,1], learning_rate=0.5)\n",
    "mlp.seed_weights()\n",
    "mlp.seed_bias()\n",
    "\n",
    "X = [[0,0],[0,1],[1,0],[1,1]]\n",
    "Y = [0,1,1,0]\n",
    "\n",
    "n = len(X)\n",
    "steps = 100000\n",
    "\n",
    "errors = np.zeros(steps)\n",
    "for i in range(steps):\n",
    "    ix = np.random.randint(0,n)\n",
    "    mlp.backpropagate(X[ix], Y[ix])\n",
    "    error = sum(np.linalg.norm(Y[k] - mlp.feed(X[k]))**2 for k in range(n))\n",
    "    errors[i] = error\n",
    "\n",
    "plt.plot(list(range(1, steps + 1)), errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0] [-0.0407064]\n",
      "[0, 0, 1] [ 0.99600742]\n",
      "[0, 1, 0] [ 0.9961289]\n",
      "[0, 1, 1] [ 0.00126819]\n",
      "[1, 0, 0] [ 0.99851061]\n",
      "[1, 0, 1] [ 0.00364614]\n",
      "[1, 1, 0] [ 0.00673528]\n",
      "[1, 1, 1] [ 0.00227173]\n"
     ]
    }
   ],
   "source": [
    "for v in X:\n",
    "    print(v, mlp.feed(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Trying out the 8-1-8 problem.\n",
    "\n",
    "mlp = MLP([4,1,4], transfer_functions=[lambda x: x, np.sin], transfer_derivatives=[lambda x: 1, np.cos], learning_rate=0.2)\n",
    "# mlp = MLP([2,1,2], learning_rate=0.2)\n",
    "mlp.seed_weights()\n",
    "mlp.seed_bias()\n",
    "\n",
    "def bitlist(n):\n",
    "    def _bitlist(n, lists):\n",
    "        if n == 0:\n",
    "            return lists\n",
    "        else:\n",
    "            return _bitlist(n-1, [l + [0] for l in lists] + [l + [1] for l in lists])\n",
    "    return _bitlist(n, [[]])\n",
    "\n",
    "X = bitlist(4)\n",
    "n = len(X)\n",
    "\n",
    "steps = 100000\n",
    "\n",
    "errors = np.zeros(steps)\n",
    "\n",
    "for i in range(steps):\n",
    "    ix = np.random.randint(0,n)\n",
    "    mlp.backpropagate(X[ix], X[ix])\n",
    "    error = 0.5 * sum(np.linalg.norm(X[k] - mlp.feed(X[k]))**2 for k in range(n))\n",
    "    errors[i] = error\n",
    "    \n",
    "plt.plot(list(range(1, steps+1)), errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79253659455644687"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0] [-0.09434568  0.17252593]\n",
      "[1, 0] [ 0.75181116  0.71195231]\n",
      "[0, 1] [ 0.40934199  0.48936508]\n",
      "[1, 1] [ 0.97931006  0.90451564]\n"
     ]
    }
   ],
   "source": [
    "for v in X:\n",
    "    print(v, mlp.feed(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
